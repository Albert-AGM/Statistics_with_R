<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Exploratory data analysis</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Statistics with R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="EDA_ames.html">EDA</a>
</li>
<li>
  <a href="final_analysis.html">Final Analysis</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Exploratory data analysis</h1>

</div>


<p>First, let us load the data and necessary packages:</p>
<pre class="r"><code>load(&quot;ames_train.Rdata&quot;)
library(MASS)
library(dplyr)
library(ggplot2)
library(BAS)
library(broom)</code></pre>
<div id="section" class="section level1">
<h1></h1>
<p>Make a labeled histogram (with 30 bins) of the ages of the houses in the data set, and describe the distribution.</p>
<pre class="r"><code># type your code for Question 1 here, and Knit
ames_train &lt;- ames_train %&gt;% 
  mutate(ages = 2019 - Year.Built) 

ggplot(ames_train,aes(ages)) +
  geom_histogram(aes(y = ..density..), bins = 30, colour = &#39;black&#39;, fill = &#39;white&#39;) +
    geom_density(fill=&quot;blue&quot;, alpha = 0.2) + 
  labs(x = &#39;Ages of the houses in years&#39;, y = &#39;Number of houses&#39;, title = &#39;Ages of the houses in the data set (Acutal year - Year built)&#39;)  + 
  scale_x_continuous(breaks = seq(0,160,10)) </code></pre>
<p><img src="EDA_ames_files/figure-html/Q1-1.png" width="672" /></p>
<pre class="r"><code>ames_train %&gt;% 
  summarise(mean = mean(ages), median = median(ages), sd = sd(ages))</code></pre>
<pre><code>## # A tibble: 1 x 3
##    mean median    sd
##   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1  46.8     44  29.6</code></pre>
<hr />
<p><strong>ANSWER:</strong> From the histogram above, we can see:</p>
<ul>
<li>The distribution of ages of the houses in the data set is right-skewed, logically we have a boundary on year 0.</li>
<li>There are many houses around 15 years old whereas the mean is 46,8 and the median 44.</li>
<li>Surprisingly there is a gap between houses of 25 and 40 years, that deserve further investigation because maybe it was a period where few houses were constructed.</li>
</ul>
<hr />
</div>
<div id="section-1" class="section level1">
<h1></h1>
<p>The mantra in real estate is “Location, Location, Location!” Make a graphical display that relates a home price to its neighborhood in Ames, Iowa. Which summary statistics are most appropriate to use for determining the most expensive, least expensive, and most heterogeneous (having the most variation in housing price) neighborhoods? Report which neighborhoods these are based on the summary statistics of your choice. Report the value of your chosen summary statistics for these neighborhoods.</p>
<pre class="r"><code># type your code for Question 2 here, and Knit

df1 &lt;- ames_train %&gt;% 
  group_by(Neighborhood) %&gt;% 
  summarise(mean = mean(price), median = median(price), sd = sd(price))

df1 %&gt;% 
  arrange(desc(median)) %&gt;% 
  select(Neighborhood, median) %&gt;% 
  slice(1:5)</code></pre>
<pre><code>## # A tibble: 5 x 2
##   Neighborhood  median
##   &lt;fct&gt;          &lt;dbl&gt;
## 1 StoneBr      340692.
## 2 NridgHt      336860 
## 3 NoRidge      290000 
## 4 GrnHill      280000 
## 5 Timber       232500</code></pre>
<pre class="r"><code>df1 %&gt;% 
  arrange(median) %&gt;% 
  select(Neighborhood, median) %&gt;% 
  slice(1:5)</code></pre>
<pre><code>## # A tibble: 5 x 2
##   Neighborhood median
##   &lt;fct&gt;         &lt;dbl&gt;
## 1 MeadowV       85750
## 2 BrDale        98750
## 3 IDOTRR        99500
## 4 OldTown      120000
## 5 Blueste      123900</code></pre>
<pre class="r"><code>df1 %&gt;% 
  arrange(desc(sd)) %&gt;% 
  select(Neighborhood, sd) %&gt;% 
  slice(1:5)</code></pre>
<pre><code>## # A tibble: 5 x 2
##   Neighborhood      sd
##   &lt;fct&gt;          &lt;dbl&gt;
## 1 StoneBr      123459.
## 2 NridgHt      105089.
## 3 Timber        84030.
## 4 Veenker       72545.
## 5 Crawfor       71268.</code></pre>
<pre class="r"><code>ggplot(ames_train, aes(Neighborhood, price / 1000)) +
  geom_boxplot(aes(colour = Neighborhood)) +
  theme(axis.text.x = element_text(angle = 90)) +
  theme(legend.position = &quot;none&quot;) +
  labs(y = &#39;Price (in thousand of dollars)&#39;, title = &#39;Price by Neighborhood&#39;)</code></pre>
<p><img src="EDA_ames_files/figure-html/Q2-1.png" width="672" /></p>
<hr />
<p><strong>ANSWER:</strong> The summary statistic most appropiate for determining the most expensive and least expensive neighborhoods is the median, because it is not very affected by outliers. The most appropiate for determining heterogeneous neighborhood is the standard deviation.</p>
<ul>
<li>Most expensive -&gt; StoneBr</li>
<li>Least expensive -&gt; MeadowV</li>
<li>Most heterogeneous -&gt; StoneBr</li>
</ul>
<hr />
</div>
<div id="section-2" class="section level1">
<h1></h1>
<p>Which variable has the largest number of missing values? Explain why it makes sense that there are so many missing values for this variable.</p>
<pre class="r"><code># type your code for Question 3 here, and Knit
sort(colSums(is.na(ames_train)), decreasing = TRUE)[1:5]</code></pre>
<pre><code>##      Pool.QC Misc.Feature        Alley        Fence Fireplace.Qu 
##          997          971          933          798          491</code></pre>
<hr />
<p><strong>ANSWER:</strong> The variable with more missing values is Pool Quality. This is because when the house does not have a swimming pool it appears on the dataset as nan. Therefore in the dataset there are 997 houses without a swimming pool.</p>
<hr />
</div>
<div id="section-3" class="section level1">
<h1></h1>
<p>We want to predict the natural log of the home prices. Candidate explanatory variables are lot size in square feet (Lot.Area), slope of property (Land.Slope), original construction date (Year.Built), remodel date (Year.Remod.Add), and the number of bedrooms above grade (Bedroom.AbvGr). Pick a model selection or model averaging method covered in the Specialization, and describe how this method works. Then, use this method to find the best multiple regression model for predicting the natural log of the home prices.</p>
<pre class="r"><code># type your code for Question 4 here, and Knit
lm_bas = bas.lm(log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr, data = ames_train,
                prior = &#39;BIC&#39;,
                modelprior = uniform())</code></pre>
<pre><code>## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts
## argument ignored</code></pre>
<pre class="r"><code>round(summary(lm_bas), 3)</code></pre>
<pre><code>##                P(B != 0 | Y)   model 1   model 2   model 3   model 4
## Intercept              1.000     1.000     1.000     1.000     1.000
## Lot.Area               1.000     1.000     1.000     1.000     1.000
## Land.SlopeMod          0.904     1.000     0.000     1.000     0.000
## Land.SlopeSev          0.904     1.000     0.000     1.000     0.000
## Year.Built             1.000     1.000     1.000     1.000     1.000
## Year.Remod.Add         1.000     1.000     1.000     1.000     1.000
## Bedroom.AbvGr          1.000     1.000     1.000     0.000     0.000
## BF                        NA     1.000     0.106     0.000     0.000
## PostProbs                 NA     0.904     0.096     0.000     0.000
## R2                        NA     0.562     0.554     0.534     0.525
## dim                       NA     7.000     5.000     6.000     4.000
## logmarg                   NA -2198.167 -2200.410 -2226.424 -2228.480
##                  model 5
## Intercept          1.000
## Lot.Area           0.000
## Land.SlopeMod      1.000
## Land.SlopeSev      1.000
## Year.Built         1.000
## Year.Remod.Add     1.000
## Bedroom.AbvGr      1.000
## BF                 0.000
## PostProbs          0.000
## R2                 0.524
## dim                6.000
## logmarg        -2236.437</code></pre>
<pre class="r"><code>print(lm_bas)</code></pre>
<pre><code>## 
## Call:
## bas.lm(formula = log(price) ~ Lot.Area + Land.Slope + Year.Built + 
##     Year.Remod.Add + Bedroom.AbvGr, data = ames_train, prior = &quot;BIC&quot;, 
##     modelprior = uniform())
## 
## 
##  Marginal Posterior Inclusion Probabilities: 
##      Intercept        Lot.Area   Land.SlopeMod   Land.SlopeSev  
##          1.000           1.000           0.904           0.904  
##     Year.Built  Year.Remod.Add   Bedroom.AbvGr  
##          1.000           1.000           1.000</code></pre>
<pre class="r"><code>image(lm_bas, rotate = F)</code></pre>
<p><img src="EDA_ames_files/figure-html/Q4-1.png" width="672" /></p>
<pre class="r"><code>ames_coef = coef(lm_bas)
ames_coef</code></pre>
<pre><code>## 
##  Marginal Posterior Summaries of Coefficients: 
## 
##  Using  BMA 
## 
##  Based on the top  33 models 
##                 post mean   post SD     post p(B != 0)
## Intercept        1.202e+01   8.831e-03   1.000e+00    
## Lot.Area         1.013e-05   1.184e-06   1.000e+00    
## Land.SlopeMod    1.251e-01   6.257e-02   9.040e-01    
## Land.SlopeSev   -4.129e-01   1.971e-01   9.040e-01    
## Year.Built       6.046e-03   3.792e-04   1.000e+00    
## Year.Remod.Add   6.789e-03   5.481e-04   1.000e+00    
## Bedroom.AbvGr    8.688e-02   1.078e-02   1.000e+00</code></pre>
<pre class="r"><code>confint(ames_coef)</code></pre>
<pre><code>##                         2.5%        97.5%          beta
## Intercept       1.200145e+01 1.203603e+01  1.201847e+01
## Lot.Area        7.730925e-06 1.236903e-05  1.012662e-05
## Land.SlopeMod   0.000000e+00 2.191468e-01  1.251455e-01
## Land.SlopeSev  -6.989198e-01 0.000000e+00 -4.128938e-01
## Year.Built      5.274440e-03 6.760363e-03  6.046237e-03
## Year.Remod.Add  5.739091e-03 7.876729e-03  6.788766e-03
## Bedroom.AbvGr   6.617872e-02 1.083440e-01  8.687950e-02
## attr(,&quot;Probability&quot;)
## [1] 0.95
## attr(,&quot;class&quot;)
## [1] &quot;confint.bas&quot;</code></pre>
<pre class="r"><code># best = which.max(lm_bas$logmarg)
# bestmodel = lm_bas$which[[best]] + 1
# plot(ames_coef, subset = c(bestmodel), ask = FALSE)

plot(confint(ames_coef, parm = 2:ames_coef$n.vars))</code></pre>
<pre><code>## Warning in arrows(x[not.deg], ci[not.deg, 1], x[not.deg], ci[not.deg,
## 2], : zero-length arrow is of indeterminate angle and so skipped</code></pre>
<p><img src="EDA_ames_files/figure-html/Q4-2.png" width="672" /></p>
<pre><code>## NULL</code></pre>
<hr />
<p><strong>ANSWER:</strong> In order to do model averaging we use the BAS package. To represent model uncertainty, we need to construct a probability distribution over all possible models where each probability provides measure of how likely the model is to happen.</p>
<p>From the information above, we can conclude that the best model include all the vairables:</p>
<p>log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr</p>
<p>The posterior probability of the best model is 0.904, and the R-squared associated is 0.562 what means that the 56% of the variance on the response variable is explained by the model.</p>
<p>The coefficients of each variable are represented in the table and plot below with a confidence interval of 95%.</p>
<hr />
</div>
<div id="section-4" class="section level1">
<h1></h1>
<p>Which home has the largest squared residual in the previous analysis (Question 4)? Looking at all the variables in the data set, can you explain why this home stands out from the rest (what factors contribute to the high squared residual and why are those factors relevant)?</p>
<pre class="r"><code># type your code for Question 5 here, and Knit
plot(lm_bas, which = 1)</code></pre>
<p><img src="EDA_ames_files/figure-html/Q5-1.png" width="672" /></p>
<pre class="r"><code>house_428 &lt;- ames_train %&gt;%
  slice(428)</code></pre>
<hr />
<p><strong>ANSWER:</strong> The home with the larges squared residual is the number 428. With the following characteristics:</p>
<ul>
<li>Lot Area: 9656</li>
<li>Land Slope: Gentle slope</li>
<li>Year Built: 1920</li>
<li>Year Remod: 1970</li>
<li>Bedrrom AbvGr: 2</li>
</ul>
<p>The residual is negative, what means we have predicted the price upper the real price of the house. Looking at the dataset we know that the house is very old and the overall quality and overall condition is poor. So it would be interesting to introduce one of both characteristics in the model to predict better its value.</p>
<hr />
</div>
<div id="section-5" class="section level1">
<h1></h1>
<p>Use the same model selection method you chose in Question 4 to again find the best multiple regression model to predict the natural log of home prices, but this time <strong>replacing Lot.Area with log(Lot.Area)</strong>. Do you arrive at a model including the same set of predictors?</p>
<pre class="r"><code># type your code for Question 6 here, and Knit
lm_bas_2 = bas.lm(log(price) ~ log(Lot.Area) + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr, data = ames_train,
                prior = &#39;BIC&#39;,
                modelprior = uniform())</code></pre>
<pre><code>## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts
## argument ignored</code></pre>
<pre class="r"><code>round(summary(lm_bas), 3)</code></pre>
<pre><code>##                P(B != 0 | Y)   model 1   model 2   model 3   model 4
## Intercept              1.000     1.000     1.000     1.000     1.000
## Lot.Area               1.000     1.000     1.000     1.000     1.000
## Land.SlopeMod          0.904     1.000     0.000     1.000     0.000
## Land.SlopeSev          0.904     1.000     0.000     1.000     0.000
## Year.Built             1.000     1.000     1.000     1.000     1.000
## Year.Remod.Add         1.000     1.000     1.000     1.000     1.000
## Bedroom.AbvGr          1.000     1.000     1.000     0.000     0.000
## BF                        NA     1.000     0.106     0.000     0.000
## PostProbs                 NA     0.904     0.096     0.000     0.000
## R2                        NA     0.562     0.554     0.534     0.525
## dim                       NA     7.000     5.000     6.000     4.000
## logmarg                   NA -2198.167 -2200.410 -2226.424 -2228.480
##                  model 5
## Intercept          1.000
## Lot.Area           0.000
## Land.SlopeMod      1.000
## Land.SlopeSev      1.000
## Year.Built         1.000
## Year.Remod.Add     1.000
## Bedroom.AbvGr      1.000
## BF                 0.000
## PostProbs          0.000
## R2                 0.524
## dim                6.000
## logmarg        -2236.437</code></pre>
<pre class="r"><code>round(summary(lm_bas_2), 3)</code></pre>
<pre><code>##                P(B != 0 | Y)   model 1   model 2   model 3   model 4
## Intercept              1.000     1.000     1.000     1.000     1.000
## log(Lot.Area)          1.000     1.000     1.000     1.000     1.000
## Land.SlopeMod          0.023     0.000     1.000     0.000     1.000
## Land.SlopeSev          0.023     0.000     1.000     0.000     1.000
## Year.Built             1.000     1.000     1.000     1.000     1.000
## Year.Remod.Add         1.000     1.000     1.000     1.000     1.000
## Bedroom.AbvGr          1.000     1.000     1.000     0.000     0.000
## BF                        NA     1.000     0.024     0.000     0.000
## PostProbs                 NA     0.977     0.023     0.000     0.000
## R2                        NA     0.603     0.606     0.591     0.593
## dim                       NA     5.000     7.000     4.000     6.000
## logmarg                   NA -2142.565 -2146.306 -2153.733 -2158.390
##                  model 5
## Intercept          1.000
## log(Lot.Area)      1.000
## Land.SlopeMod      0.000
## Land.SlopeSev      0.000
## Year.Built         1.000
## Year.Remod.Add     0.000
## Bedroom.AbvGr      1.000
## BF                 0.000
## PostProbs          0.000
## R2                 0.536
## dim                4.000
## logmarg        -2217.738</code></pre>
<p><strong>ANSWER:</strong> We do not arrive to the same model. This time the predictor Land Slope is not included in the model. It goes from having a probability of being included in the model from 0.904 to 0.023. Notice with this new model the R-squared increases to 0.603, therefore this model is able to explain better the variance on the response variable.</p>
<hr />
</div>
<div id="section-6" class="section level1">
<h1></h1>
<p>Do you think it is better to log transform Lot.Area, in terms of assumptions for linear regression? Make graphs of the predicted values of log home price versus the true values of log home price for the regression models selected for Lot.Area and log(Lot.Area). Referencing these two plots, provide a written support that includes a quantitative justification for your answer in the first part of question 7.</p>
<pre class="r"><code># type your code for Question 7 here, and Knit
predicted_values &lt;- predict(lm_bas, estimator = &#39;BPM&#39;, se.fit = TRUE)

predicted_values_log &lt;- predict(lm_bas_2, estimator = &#39;BPM&#39;, se.fit = TRUE)

ames_train$predicted_price = predicted_values$fit
ames_train$predicted_price_log = predicted_values_log$fit


ggplot(ames_train, aes(predicted_price, log(price))) +
  geom_point()</code></pre>
<p><img src="EDA_ames_files/figure-html/Q7-1.png" width="672" /></p>
<pre class="r"><code>ggplot(ames_train, aes(predicted_price_log, log(price))) +
  geom_point()</code></pre>
<p><img src="EDA_ames_files/figure-html/Q7-2.png" width="672" /></p>
<pre class="r"><code>par(mfrow = c(2,2))

plot(lm_bas)</code></pre>
<p><img src="EDA_ames_files/figure-html/Q7-3.png" width="672" /></p>
<pre class="r"><code>plot(lm_bas_2)</code></pre>
<p><img src="EDA_ames_files/figure-html/Q7-4.png" width="672" /></p>
<hr />
<p><strong>ANSWER:</strong> In both models, the linearity and variance of the residuals are met. In the model with log(Lot.Area) the R-squared is higher and the model also has fewer predictors (the model is simplest). So we can conclude that is better to log transform Lot.Area.</p>
<hr />
<div id="section-7" class="section level3">
<h3></h3>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
